% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amssymb}
\usepackage{amsmath}
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Econometrics HW3}
\author{Michael B. Nattinger\footnote{I worked on this assignment with my study group: Alex von Hafften, Andrew Smith, and Ryan Mather. I have also discussed problem(s) with Emily Case, Sarah Bass, and Danny Edgel.}}

\begin{document}
\maketitle

\section{7.28}
\subsection{Part A}
\begin{center}
\input{table_hw4}
\end{center}
\subsection{Part B}
The derivative of log wage with respect to education is $\beta_1$ and the derivative of log wage with respect to experience is $\beta_2 + \beta_3 exp/50$ so $\theta = \frac{\beta_1}{\beta_2 + \beta_3 exp/50}$. Therefore, for 10 experience, our estimate implied by our regressions is the following:

\begin{align}
\hat{\theta} &= \frac{\hat{\beta_1}}{\hat{\beta_2} + \hat{\beta}_3 exp/50}\\
&=  \frac{0.1443}{0.0426 - 0.0951 (10)/50}\\
&= 6.109
\end{align}

\subsection{Part C}
We can find the asymptotic standard error as the square root of the asymptotic variance of the $\hat{\theta}$ estimator, which we can calculate through the delta method:

\begin{align*}
s(\hat{\theta}) &= \sqrt{g'(\beta)'V g'(\beta) },
\end{align*}
where V is the asymptotic covariance matrix of the non-intercept coefficients, and $g(\beta) = \frac{\hat{\beta_1}}{\hat{\beta_2} + \hat{\beta}_3 exp/50}$. Then,
\begin{align*}
g'(\beta) &= \colvec{3}{\frac{1}{\beta_2 + \beta_3 exp/50}}{\frac{-\beta_1}{(\beta_2 + \beta_3 exp/50)^2}}{\frac{-\beta_1 exp/50}{(\beta_2 + \beta_3 exp/50)^2}}
\end{align*}

We can calculate an estimate for $s(\hat{\theta})$ by plugging in OLS estimates of $\beta$ and our robust standard error matrix we used in Part A. Our 90\% CI is $[\hat{\theta} - 1.645s(\hat{\theta}),\hat{\theta} + 1.645s(\hat{\theta})]$.

\subsection{Part D}
 Our computed $\hat{\theta}$, $s(\hat{\theta})$, and confidence interval are the following:

\begin{align*}
\hat{\theta} &= 6.109 \\
s(\hat{\theta}) &= 1.6178 \\
CI &= [4.4912,7.7269]
\end{align*}.

\section{8.1}
Let $\beta = [\beta_1,\beta_2]$ be the CLS estimator of $Y = X_1'\beta_1 + X_2' \beta_2 +e$ subject to the constraint that $\beta_2 = 0.$ From definition (8.3),

\begin{align*}
\beta &= \argmin_{\beta_2 = 0}  (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2)\\
\Rightarrow \mathcal{L} &= (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2) + \lambda'(\beta_2 - 0)\\
\Rightarrow 0&= -2X_1'(Y-X_1\beta_1 - X_2 \beta_2)\\
\Rightarrow X_1'Y &= (X_1'X_1)\beta_1 \\
\Rightarrow \beta_1 &= (X_1'X_1)^{-1}X_1'Y.
\end{align*}

\section{8.3}
%\begin{align*} % whoops solved the wrong problem on accident
%\beta &= \argmin_{\beta_2 = c}  (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2)\\
%\Rightarrow \mathcal{L} &= (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2) + \lambda'(\beta_2 - c)\\
%\Rightarrow 0&= -2X_1'(Y-X_1\beta_1 - X_2 \beta_2)\\
%\Rightarrow X_1'X_1\beta_1 &= X_1'(Y - X_2c)\\
%\Rightarrow \beta_1 &= (X_1'X_1)^{-1}X_1'(Y - X_2c)
%\end{align*}
\begin{align*}
\beta &= \argmin_{\beta_1 = -\beta_2}  (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2)\\
\Rightarrow \mathcal{L} &= (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2) + \lambda'(\beta_2 + \beta_1)\\
\Rightarrow 0&= -2X_1'(Y-X_1\beta_1 - X_2 \beta_2)+\lambda\\
\Rightarrow 0&= -2X_2'(Y-X_1\beta_1 - X_2 \beta_2)+\lambda\\
\Rightarrow 0&= (X_1 - X_2)'(Y-X_1\beta_1 + X_2 \beta_1)\\
\Rightarrow \beta_1 &= -\beta_2 =( (X_1 - X_2)'(X_1 - X_2))^{-1}(X_1 - X_2)'Y
\end{align*}
\section{8.4(a)}
Let $Z = X$
\begin{align*}
\alpha &= \argmin_{\beta = 0} (Y-X\beta - \alpha)'(Y-X\beta - \alpha)\\
\Rightarrow \mathcal{L} &=  (Y-X\beta - \alpha)'(Y-X\beta - \alpha) + \lambda'(\beta)\\
\Rightarrow 0&=-\vec{1}(Y-X\beta - \alpha) \\
\Rightarrow \alpha &= \frac{1}{n}\vec{1}'Y = \frac{1}{n}\sum_iY_i
\end{align*}
\section{8.22}
\subsection{Part A}
\begin{align*}
\tilde{\beta} &= \argmin_{2\beta_2 = \beta_1}  (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2)\\
\Rightarrow \mathcal{L} &= (Y-X_1\beta_1 -X_2\beta_2)'(Y-X_1\beta_1 -X_2\beta_2) + \lambda'(2\beta_2 - \beta_1)\\
\Rightarrow 0&= -2X_1'(Y-X_1\beta_1 - X_2 \beta_2)+\lambda\\
\Rightarrow 0&= -2X_2'(Y-X_1\beta_1 - X_2 \beta_2)+2\lambda\\
\Rightarrow 0&= (2X_1 + X_2)'(Y-X_12\beta_2 - X_2 \beta_2)\\
\Rightarrow \tilde{\beta}_2&= ((2X_1 +X_2)'(2X_1 +X_2))^{-1}(2X_1 +X_2)'Y\\
\Rightarrow \tilde{\beta}_1&= 2\tilde{\beta}_2
\end{align*}
\subsection{Part B}
\begin{align*}
\sqrt{n}(\tilde{\beta}_2 - \beta_2) &= 2\sqrt{n}((2X_1 +X_2)'(2X_1 +X_2))^{-1}(2X_1 +X_2)'e\\
&=2 (\frac{1}{n}\sum_i (2X_{1,i} +X_{2,i})^{2})^{-1} \frac{1}{\sqrt{n}}\sum_i (2X_{1,i} +X_{2,i})e_i\\
&\Rightarrow N\left(0,\frac{E[ (2X_{1,i} +X_{2,i})^2e_i^2]}{E[(2X_{1,i} +X_{2,i})^{2}]^2} \right)
\end{align*}

\section{9.1} %oof
Let $\hat{\beta}$ be the OLS regression of $y$ on $X$. Similarly consider the regression with the restriction $\beta_{k+1} = 0 := \tilde{\beta}.$ 
\begin{align*}
\tilde{\beta} &= \hat{\beta} - (X'X)^{-1}[\vec{0}_k 1]' ([\vec{0}_k 1](X'X)^{-1}[\vec{0}_k 1]')^{-1}[\vec{0}_k 1]\hat{\beta}\\
&= \hat{\beta} - (X'X)^{-1}[\vec{0}_k 1]' ([(X'X)^{-1}]_{k+1,k+1})^{-1}\hat{\beta}_{k+1}.\\
\tilde{\epsilon} &= y - X\tilde{\beta}\\
&= \hat{\epsilon} - X(\tilde{\beta} - \hat{\beta})\\
\Rightarrow \tilde{\epsilon}'\tilde{\epsilon} &= \hat{\epsilon}'\hat{\epsilon} + (\tilde{\beta} - \hat{\beta})'X'X(\tilde{\beta} - \hat{\beta}) - \hat{\epsilon}'X(\tilde{\beta} - \hat{\beta}) - (\tilde{\beta} - \hat{\beta})'X'\tilde{\epsilon}\\
&= \hat{\epsilon}'\hat{\epsilon} +\hat{\beta}_{k+1} ([(X'X)^{-1}]_{k+1,k+1})^{-1}[\vec{0}_k 1](X'X)^{-1}X'X(X'X)^{-1}[\vec{0}_k 1]' ([(X'X)^{-1}]_{k+1,k+1})^{-1}\hat{\beta}_{k+1}\\
&= \hat{\epsilon}'\hat{\epsilon}  + \hat{\beta}_{k+1} ([(X'X)^{-1}]_{k+1,k+1})^{-1}[\vec{0}_k 1](X'X)^{-1}[\vec{0}_k 1]' ([(X'X)^{-1}]_{k+1,k+1})^{-1}\hat{\beta}_{k+1}\\
&= \hat{\epsilon}'\hat{\epsilon}  + \hat{\beta}_{k+1} ([(X'X)^{-1}]_{k+1,k+1})^{-1}[(X'X)^{-1}]_{k+1,k+1} ([(X'X)^{-1}]_{k+1,k+1})^{-1}\hat{\beta}_{k+1}\\
&= \hat{\epsilon}'\hat{\epsilon}  + \frac{\hat{\beta}_{k+1}^2}{[(X'X)^{-1}]_{k+1,k+1}}.
\end{align*} 

Consider the adjusted R-sq for unrestricted and restricted regressions, $R^2_{k+1},R^2_k$. Define $E:= \frac{1}{n--1}(y_i - \bar{y})^2.$

\begin{align*}
R^2_{k+1}>R^2_k &\iff 1 - \frac{\frac{1}{n-k-1}\hat{\epsilon}'\hat{\epsilon}}{E}>1 - \frac{\frac{1}{n-k}\tilde{\epsilon}'\tilde{\epsilon}}{E} \\
&\iff\frac{1}{n-k-1}\hat{\epsilon}'\hat{\epsilon}<\frac{1}{n-k}\tilde{\epsilon}'\tilde{\epsilon}\\
&\iff(n-k-1)( \tilde{\epsilon}'\tilde{\epsilon} - \hat{\epsilon}'\hat{\epsilon}) > \tilde{\epsilon}'\tilde{\epsilon}\\
&\iff \frac{\hat{\beta}^2_{k+1}}{s^2[(X'X)^{-1}]_{k+1,k+1}}>1\\
&\iff \frac{\hat{\beta}^2_{k+1}}{s(\hat{\beta}_{k+1})^2}>1\\
&\iff\left|\frac{\hat{\beta}_{k+1}}{s(\hat{\beta}_{k+1})^2}\right|>1\\
&\iff |T_{k+1}|>1.
\end{align*}

\section{9.2}
\subsection{Part A}
$\hat{\beta}_1,\hat{\beta}_2$ are OLS estimates of the coefficients, so $\sqrt{n}(\hat{\beta}_1 - \beta_1 ) \rightarrow_d N(0,V_1),\sqrt{n}(\hat{\beta}_1 - \beta_2 ) \rightarrow_d N(0,V_2)$ where $V_j = E[x_{j.i}x_{j.i}']^{-1}E[x_{j.i}x_{j.i}'e_{j,i}^{2}]E[x_{j.i}x_{j.i}']^{-1}$.

\begin{align*}
\sqrt{n}\colvec{2}{\hat{\beta}_1 - \beta_1}{\hat{\beta}_2 - \beta_2} &= \begin{pmatrix} (\frac{1}{n}\sum_{i=1}^n x_{1,i}x_{1,i}')^{-1} & 0\\ 0 & (\frac{1}{n}\sum_{i=1}^n x_{2,i}x_{2,i}')^{-1} \end{pmatrix}\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\colvec{2}{x_{i,1}e_{i,1}}{x_{i,2}e_{i,2}}
\end{align*}

\begin{align*}
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\colvec{2}{x_{i,1}e_{i,1}}{x_{i,2}e_{i,2}} &\rightarrow_d N\left(0,\begin{pmatrix} V_1 & 0 \\ 0 & V_2\end{pmatrix}\right)
\end{align*}

By CMT, $\sqrt{n}((\hat{\beta}_1 - \hat{\beta}_2)-(\beta_1 - \beta_2)) \rightarrow_d N(0,V_1+V_2)$.

\subsection{Part B}
%Under the null hypothesis that $\beta_1 = \beta_2, \sqrt{n}((\hat{\beta}_1 - \hat{\beta}_2)-(\beta_1 - \beta_2)) = \sqrt{n}(\hat{\beta}_1 - \hat{\beta}_2)$ so we can define $\theta:= \beta_1 - \beta_2,\hat{\theta} = \hat{\beta}_1 - \hat{\beta}_2$, our $se = \frac{\sqrt{n}}{\hat{V}_1+\hat{V}_2}$ where $\hat{V}_i$ is the OLS estimates of the coefficient covariance matrix of regression $i$. Then $t = \frac{\hat{\theta}}{se}$ is our test statistic.
We have a multidimensional restriction so the test statistic we should use for $H_{0}: \beta_1 = \beta_2$ is the Wald statistic $W_n = n(\hat{\beta}_1 - \hat{\beta}_2)'(\hat{V}_1 + \hat{V}_2)^{-1}(\hat{\beta}_1 - \hat{\beta}_2)$.


\subsection{Part C}
%By construction, this test statistic $t$ is asymptotically drawn from a $N(0,1)$.
Since $\hat{V}_j \rightarrow_p V_j$, from (a) $W_n \rightarrow_d \chi^2_k$.
\section{9.4}
\subsection{Part A}
\begin{align*}
P(W<c_1\cup W>c_2) &= P(W<c_1) + P(W>c_2) \rightarrow_p F(c_1) + (1-F(c_2)) = \alpha/2 + \alpha/2\\
&= \alpha.
\end{align*}
\subsection{Part B}
This is a bad test because if $W<c_1$ then $\theta$ is very close to 0. If the null hypothesis is true then drawing a $W<c_1$ is just a draw of $\theta$ near its true mean, 0. We should not be rejecting the null in this case. Rejecting in this case results in a loss of power.  
\section{9.7}
We are testing the null hypothesis of $20 = 40\beta_1 + 1600 \beta_2\Rightarrow 1/2 = \beta_1 + 40 \beta_2$. We can define $\theta =  \beta_1 + 40 \beta_2 - 1/2$. Then, under the null hypothesis, $\sqrt{n}(\hat{\theta} - \theta) \rightarrow_d N(0,V)$ where $V = \begin{pmatrix} 1 & 40\end{pmatrix} V_{\beta} \colvec{2}{1}{40}$, where $V_{\beta}$ is the asymptotic covariance matrix of $\beta$.  To test the hypothesis, we can calculate $\hat{\theta}$ by plugging in our OLS estimates of $\beta$, and plug in our OLS estimates of the covariance matrix $\hat{V}_{\beta}$, and then the SE of our test is $se = \sqrt{\frac{\hat{V}}{n}}$, our test statistic is $t=\frac{\hat{\theta}}{se}$. We can then reject the null hypothesis if $|t|>q_{1-\alpha/2}$ where $q_{1-\alpha/2}$ is the $1-\alpha/2$ quantile of a standard normal, and $\alpha$ is the size of the test.
\end{document}
