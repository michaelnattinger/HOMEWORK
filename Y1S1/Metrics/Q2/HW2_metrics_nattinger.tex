% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amssymb}
\usepackage{amsmath}
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Econometrics HW2}
\author{Michael B. Nattinger\footnote{I worked on this assignment with my study group: Alex von Hafften, Andrew Smith, and Ryan Mather. I have also discussed problem(s) with Emily Case, Sarah Bass, and Danny Edgel.}}

\begin{document}
\maketitle

\section{3.2}
\begin{align*}
\hat{\beta}_{ols} &= (X'X)^{-1}X'Y\\
\hat{\beta}_{mix} &= (Z'Z)^{-1}Z'Y \\
&= (C'X'XC)^{-1}C'X'Y\\
&= C^{-1}(X'X)^{-1}C'^{-1}C'X'Y\\
&= C^{-1}\hat{\beta}_{ols}\\
\hat{\epsilon}_{ols} &= (I -X(X'X)^{-1}X')Y\\
\hat{\epsilon}_{mix} &= (I -XCC^{-1}(X'X)^{-1}X')Y\\
&= (I -X(X'X)^{-1}X')Y\\
&= \hat{\epsilon}_{ols}
\end{align*}
\section{3.5}
\begin{align*}
\hat{\epsilon} &= Y-X'(X'X)^{-1}X'Y = (I - X(X'X)^{-1}X')Y\\
\hat{\beta}_e &= (X'X)^{-1}X'\hat{\epsilon}\\
&=  (X'X)^{-1}X'(I - X(X'X)^{-1}X')Y\\
&= (X'X)^{-1}X'Y - (X'X)^{-1}X'X(X'X)^{-1}X'Y\\
&= (X'X)^{-1}X'Y - (X'X)^{-1}X'Y\\
&= 0.
\end{align*}
\section{3.6}
\begin{align*}
\hat{\beta}_{\hat{Y}} &= (X'X)^{-1}X'\hat{Y}\\
&=(X'X)^{-1}X'X(X'X)^{-1}X'Y\\
&= (X'X)^{-1}X'Y \\
&= \hat{\beta}_{ols}
\end{align*}
\section{3.7}
Note that $X_1 = X\Gamma$ where $\Gamma = \colvec{2}{I_{n_1}}{\bar{0}}$ where $\bar{0}$ is an $n_2 \times n_1$ vector of zeros. Then,
\begin{align*}
PX_1 &= X(X'X)^{-1}X'X_1\\
&= X(X'X)^{-1}X'X\Gamma\\
&= X\Gamma\\
&= X_1.
\end{align*}

\begin{align*}
MX_1 &= (I - X(X'X)^{-1}X')X\Gamma\\
&= (X - X(X'X)^{-1}X'X)\Gamma\\
&= (X-X)\Gamma\\
&= 0.
\end{align*}

\section{3.11}
Let $X$ contain a constant.
\begin{align*}
\frac{1}{n}\sum_{i=1}^n \hat{Y}_i &= \frac{1}{n}\sum_{i=1}^n Y_i - \hat{\epsilon}_i\\
&= \frac{1}{n}\sum_{i=1}^n Y_i  - \frac{1}{n}\sum_{i=1}^n \hat{\epsilon}_i \\
&= \frac{1}{n}\sum_{i=1}^n Y_i -\frac{1}{n}\vec{1}'\hat{\epsilon}\\
&=  \frac{1}{n}\sum_{i=1}^n Y_i
\end{align*}

where $\frac{1}{n}\sum_{i=1}^n \hat{\epsilon}_i  = \frac{1}{n}\vec{1}'\hat{\epsilon}  = 0$ because X contains a column of ones.

\section{3.12}
Equation (3.53) cannot be estimated by OLS because $D_1 + D_2 = \vec{1}$ (a vector containing 1 in every element), and therefore $X'X$ is not invertible (perfect collinearity with the constant term).
\subsection{Part A}
Equations (3.54) and (3.55) contain the same information, since  $D_1 + D_2 = \vec{1}$, and so the $\hat{Y}$ from each regression would be identical. Ergo,
\begin{align*}
D_1\alpha_1 + D_2 \alpha_2 + e &= (\vec{1} - D_2)\alpha_1 + D_2 \alpha_2\\
&= \vec{\alpha_1 } +D_2(\alpha_2 - \alpha_1)
\end{align*}

Therefore, the regressions are the same with $\mu = \alpha_1$ and $\phi = \alpha_2 - \alpha_1.$
\subsection{Part B}
\begin{align*}
\vec{1}'D_1 &= \sum_{i=1}^n 1\{ \text{person $i$ is a man}\}\\
&=n_1, \\
\vec{1}'D_2 &= \sum_{i=1}^n 1\{ \text{person $i$ is a woman}\}\\
&=n_2.
\end{align*}
\section{3.13}
\subsection{Part A}
Let $X = [D_1 D_2]$. Order our observations such that the first $n_1$ observations are men and the rest of the observations are women, then $X'X = \begin{pmatrix} \vec{1}_{n_1}'\vec{1}_{n_1} & \bar{0} \\ \bar{0} & \vec{1}_{n_2}'\vec{1}_{n_2}  \end{pmatrix}$
\begin{align*}
\colvec{2}{\hat{\gamma}_1}{\hat{\gamma}_2} &= \begin{pmatrix} \vec{1}_{n_1}'\vec{1}_{n_1} & \bar{0} \\ \bar{0} & \vec{1}_{n_2}'\vec{1}_{n_2}  \end{pmatrix}^{-1} \colvec{2}{\sum_{i=1}^{n_1}y_i}{\sum_{i=n_1+1}^{n_1+n_2}y_i} \\
&= \colvec{2}{\frac{1}{n_1}\sum_{i=1}^{n_1}y_i}{\frac{1}{n_2}\sum_{i=n_1+1}^{n_1+n_2}y_i}\\
&= \colvec{2}{\bar{Y}_1}{\bar{Y}_2}
\end{align*}
\subsection{Part B}
The first transformation simplifies to $Y^{*} = \hat{u}$, in other words $Y^{*}$ is the deviation from average for men and women.

The second transformation similarly transforms the $X$ data, so $X^{*}$ is the residual of the following regression: $X = D_1 b_1 + D_2 b_2$, which we know from Part A will yield $b_1 = \bar{X}_1, b_2 = \bar{X}_2.$ $X^{*}$ then is a matrix of regressors transformed to be in deviations from the average for whatever gender the individual identifies as.

\subsection{Part C}
\begin{align*}
\tilde{\beta} &= (X'^{*}X^{*})^{-1} X'^{*}Y^{*}\\
&=(XM_DX)^{-1}X'M_DY\\
\hat{\beta} &= (XM_DX)^{-1}X'M_DY\\
&= \tilde{\beta}
\end{align*}
where we solved for $\hat{\beta}$ via theorem 3.4.
\section{3.16}
Let $X = [X_1 X_2], \hat{\beta} = [\hat{\beta}_1' \hat{\beta}_2' ]', \hat{\beta}^{*} = [\tilde{\beta}_1' \vec{0}_{n_2}']'$ where $\vec{0}_{n_2}$ is the $n_2$ sized matrix of zeros.
\begin{align*}
R_2^2 &= 1 - \frac{\sum_{i=1}^n\hat{e}_i^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\\
&= 1 - \frac{\hat{\epsilon}'\hat{\epsilon}}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\\
&= 1 - \frac{(Y - X\hat{\beta})'(Y - X\hat{\beta})}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\\
&\geq 1 -  \frac{(Y - X\hat{\beta}^{*})'(Y - X\hat{\beta}^{*})}{\sum_{i=1}^n (Y_i - \bar{Y})^2} \\
&= R_1^2,
\end{align*}
where the inequality comes from the fact that OLS minimizes the sum of squared residuals.

Yes, if $X_2$ is orthogonal to $Y$ then $X_2'Y = 0 \Rightarrow \hat{\beta}_2 = 0 \Rightarrow \tilde{\beta}  = \hat{\beta}  \Rightarrow R^2_2 = R^2_1.$
\section{3.21}

If one or both of $X_1,X_2$ is orthogonal to $Y$, or if $X_1, X_2$ are orthogonal to each other, then $\tilde{\beta}_1 = \hat{\beta}_1, \tilde{\beta}_2 = \hat{\beta}_2.$

The first condition is nearly immediate, as whichever regressor is orthogonal will have estimated coefficients of 0 in both equations, and the equation with both regressors included reduces to the "one regressor at a time" estimator equation, so the coefficients in both have the same estimated value. Moreover, if both regressors are orthogonal to $Y$ then all of the coefficient estimates will be 0.

Now we concern ourselves with the final case, where $X_1,X_2$ are orthogonal. Then, by theorem 3.4 we have that:
\begin{align*}
\hat{\beta}_1 &= (X'_1M_2X_1)^{-1}(X'_1 M_2 Y)\\
 &=  ((M_2X_1)'(M_2X_1))^{-1}((M_2X_1)' Y) \\
&= (X_1'X_1)^{-1}(X_1' Y)\\
&= \tilde{\beta}_1
\end{align*}
By symmetry, the same condition ensures $\hat{\beta}_2 =  \tilde{\beta}_2$
\section{3.22}
\begin{align*}
\tilde{\beta} &= (X_1'X_1)^{-1}X_1'Y\\
\tilde{u} &= Y -X_1\tilde{\beta}\\
\tilde{\beta}_2 &= (X_2'X_2)^{-1}X_2'\tilde{u}\\
&= (X_2'X_2)^{-1}X_2'(Y -X_1\tilde{\beta}_1)\\
\hat{\beta}_2 &= (X_2'X_2)^{-1}X_2'(Y -X_1\hat{\beta}_1)
\end{align*}
Therefore, this is only the case when $\tilde{\beta}_1=\hat{\beta}_1$. As we showed in the previous problem, this occurs when $X_1,X_2$ are orthogonal (or when one (or both) of the regressors is orthogonal to $Y$).
\section{3.23}
The residuals are the same from both equations, which I will show below, and therefore the residual variance estimates, a function of the estimated residuals, are the same from both regressions. Therefore, $\hat{\sigma}^2 = \tilde{\sigma}^2.$

Now we will show that the residuals are the same.
\begin{align*}
\tilde{\beta}_2 &= ((X_2-X_1)'M_1(X_2-X_1))^{-1} ((X_2 -X_1)'M_1Y)\\
&= (X_2'X_2)^{-1}X_2'Y \\
&= \hat{\beta}_2.\\
\tilde{\beta}_1 &= (X_1'X_1)^{-1}X_1' (Y-(X_2 - X_1)\tilde{\beta}_2) \\
&=  (X_1'X_1)^{-1}X_1'Y -  (X_1'X_1)^{-1}X_1' (X_2 - X_1)\tilde{\beta}_2\\
&= (X_1'X_1)^{-1}X_1'(Y - X_2\hat{\beta}_2) +  (X_1'X_1)^{-1}X_1'X_1\hat{\beta}_2\\
&= \hat{\beta}_1 + \hat{\beta}_2.\\
\Rightarrow \tilde{\epsilon} &= X_1\tilde{\beta}_1 + (X_2 - X_1)\tilde{\beta}_2\\
&= X_1(\hat{\beta}_1 + \hat{\beta}_2) +(X_2 - X_1)\hat{\beta}_2\\
&= X_1\hat{\beta}_1 + X_2 \hat{\beta}_2\\
&= \hat{\epsilon}.
\end{align*}

\section{Question 7}
\subsection{Part A}
\begin{align*}
E[\hat{\beta}|X] &= E[(X'X)^{-1}X'Y|X] \\
&=  (X'X)^{-1}X'E[Y|X] \\
&=  (X'X)^{-1}X'X\beta \\
&= \beta\\
\Rightarrow E[\hat{\beta}_1|X] &= \beta_1
\end{align*}
\subsection{Part B}
\begin{align*}
E[\hat{\hat{\beta}}_1|X] &= E[(X_1'X_1)^{-1}X_1'\hat{Y}|X]\\
&= E[(X_1'X_1)^{-1}X_1'X\hat{\beta}|X]\\
&= E[(X_1'X_1)^{-1}X_1'X(X'X)^{-1}X'Y|X]\\
&= (X_1'X_1)^{-1}X_1'X(X'X)^{-1}X'E[Y|X]\\
&= (X_1'X_1)^{-1}X_1'X(X'X)^{-1}X'X\beta\\
&=  (X_1'X_1)^{-1}X_1'X\beta\\
&= (X_1'X_1)^{-1}X_1'(X_1 \beta_1 + X_2 \beta_2)\\
&= \beta_1 +  (X_1'X_1)^{-1}X_1'X_2 \beta_2
\end{align*}
This is equal to $\beta_1$ if either $\beta_2=0$ or $X_1,X_2$ are guaranteed to be orthogonal (so $X_1'X_2=0$).
\subsection{Part C}
\begin{align*}
\tilde{\tilde{\beta}} &= (X'X)^{-1}X'\tilde{Y}\\
&= (X'X)^{-1}X'X_1\tilde{\beta}_1\\
&= \colvec{2}{I_{k_1}}{\bar{0}}\tilde{\beta}_1
\end{align*}
\subsection{Part D}
Let $\tilde{\tilde{Y}} = X\tilde{\tilde{\beta}}, \tilde{\epsilon} = \tilde{Y} - \tilde{\tilde{Y}} $.
\begin{align*}
\tilde{\tilde{Y}} &=  X\tilde{\tilde{\beta}}\\
&= X \colvec{2}{I_{k_1}}{\bar{0}}\tilde{\beta}_1\\
&= X_1\tilde{\beta}_1\\
&= \tilde{Y}\\
\Rightarrow \tilde{\epsilon} &= 0\\
\Rightarrow R^2 &= 1 - \frac{\tilde{\epsilon}'\tilde{\epsilon}}{\sum_{i=1}^{n}(\tilde{Y}_i - \bar{\tilde{Y}})^2}\\
&=1 - \frac{0}{\sum_{i=1}^{n}(\tilde{Y}_i - \bar{\tilde{Y}})^2}\\
&= 1.
\end{align*}
\subsection{Part E}

\begin{align*}
Var(\tilde{\tilde{\beta}}|X) &= Var( \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1}X_1'Y|X) \\
&=  \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1}X_1'Var[Y|X]( \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1}X_1')'\\
&= \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1}X_1'\sigma^2 I X_1(X_1'X_1)^{-1} \colvec{2}{I_{k_1}}{\bar{0}}' \\
&= \sigma^2 \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1}X_1'X_1(X_1'X_1)^{-1} \colvec{2}{I_{k_1}}{\bar{0}}' \\
&=  \sigma^2 \colvec{2}{I_{k_1}}{\bar{0}}(X_1'X_1)^{-1} \colvec{2}{I_{k_1}}{\bar{0}}' \\
&= \begin{pmatrix}\sigma^2 (X_1'X_1)^{-1} & \bar{0} \\ \bar{0} & \bar{0} \end{pmatrix}
\end{align*}
\end{document}
