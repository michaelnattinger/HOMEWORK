% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{amssymb}
\usepackage{amsmath}
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\usepackage{bbm}
\usepackage{endnotes}

\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Econometrics HW1}
\author{Michael B. Nattinger\footnote{I worked on this assignment with my study group: Alex von Hafften, Andrew Smith, and Ryan Mather. I have also discussed problem(s) with Emily Case, Sarah Bass, Katherine Kwok, and Danny Edgel.}}

\begin{document}
\maketitle

\section{Question 1}
\subsection{Part i}
$\beta_0$ are the true coefficients of the conditional expectation function of $Y$ conditional on $X$:
\begin{align*}
E[Y|X] &= E[X'\beta_0 \cdot U|X]\\
&= X'\beta_0E[U|X]\\
&= X'\beta_0.
\end{align*}
\subsection{Part ii}
Define $\bar{U}:= X'\beta_0 (U-1)$. Then,
\begin{align*}
Y &= X'\beta_0 \cdot U\\
&= X'\beta_0 \cdot U - X'\beta_0 + X'\beta_0 \\
&= X'\beta_0 +\bar{U}.
\end{align*}
Moreover, $E[\bar{U}|X] = E[ X'\beta_0 (U-1)|X] = X'\beta_0(E[U|X] - 1) = 0$.
\subsection{Part iii}
Assume $\beta = \beta_0.$ Then, by the conditioning theorem,
\begin{align*}
E[X(Y-X'\beta)] &= E[X(Y-X'\beta_0)]\\
&=  E[XE[Y-X'\beta_0|X]]\\
&= E[XE[X'\beta_0 + \bar{U}-X'\beta_0|X]]\\
&= E[XE[\bar{U}|X]]\\
&=0.
\end{align*}

Now, assume instead that $E[X(Y-X'\beta)] = 0.$ Then, again using the conditioning theorem,
\begin{align*}
0 &= E[X(Y-X'\beta)] \\
&= E[XE[Y-X'\beta|X]] \\
&= E[XE[X'\beta_0 +\bar{U}-X'\beta|X]] \\
&=  E[XE[X'(\beta_0 -\beta)|X]] \\
&= E[XX'](\beta_0 -\beta)
\end{align*}
We know $E[XX']$ is invertible so it must be the case that $(\beta_0 -\beta) = 0\Rightarrow \beta_0 = \beta$.

We have proven both directions of the iff, ergo $E[X(Y-X'\beta)] = 0$ iff $\beta = \beta_0$. We can now define our method of moments estimator $\hat{\beta}^{MM}$ to be the unique solution to the following equation:
\begin{align*}
\frac{1}{n}\sum_{i=1}^n X_i(Y_i-X_i'\beta) &= 0\\
\Rightarrow \frac{1}{n}\sum_{i=1}^n X_iY_i &= \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)\hat{\beta}^{MM}\\
\Rightarrow  \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i &=\hat{\beta}^{MM} \\
\Rightarrow \hat{\beta} &= \hat{\beta}^{MM}.
\end{align*}
Therefore, the OLS estimator is a method of moments estimator.
\subsection{Part iv}
By application of various given definitions,
\begin{align*}
E[\hat{\beta}|X_1,\dots,X_n] &= E\left[\left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\Big|X_1,\dots,X_n\right]\\
&= \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iE[Y_i|X_1,\dots,X_n]\\
&= \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iE[X_i'\beta_0\cdot U_i|X_1,\dots,X_n]\\
&= \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iX_i'\beta_0 E[U_i|X_1,\dots,X_n]\\
&= \beta_0.
\end{align*}
Therefore, the OLS estimator is conditionally unbiased.

\subsection{Part v}
By the LLN and CMT, the following are true as $n\rightarrow \infty$:
\begin{align*}
 \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1} &\rightarrow_p E[XX']^{-1},\\
\frac{1}{n}\sum_{i=1}^n X_i Y_i &= \frac{1}{n}\sum_{i=1}^n X_i (X_i'\beta_0 + \bar{U})\\
&= \frac{1}{n}\left(\sum_{i=1}^nX_i X_i' \right)\beta_0 +  \frac{1}{n}\left(\sum_{i=1}^nX_i \bar{U}\right)\\
&\rightarrow_p E[XX']\beta_0 + E[X \bar{U}]\\
&=  E[XX']\beta_0 + E[X E[\bar{U}|X]]\\
&=  E[XX']\beta_0,
\end{align*}
where we have applied the conditioning theorem in the second-to-last line. By further application of the CMT, as $n\rightarrow \infty$,
\begin{align*}
 \left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i &\rightarrow_p E[XX']^{-1}E[XX']\beta_0\\
&= \beta_0.
\end{align*}

\section{Question 2}
\subsection{Part i}
We can immediately apply LLN and CMT to show convergence in probability as $n \rightarrow \infty$ to the following statistics:
\begin{align*}
\frac{1}{n}\sum_{i=1}^n X_i^3, \\
\frac{\sum_{i=1}^n X_i^3}{\sum_{i=1}^n X_i^2}\\
\end{align*}

$\max_{1\leq i\leq n} \{X_i\}$ does not always converge in probability. For example, in part (iv) of this question we prove that, for $X_i \sim exponential(1)$, this function has no probability limit.\footnote{The only part of this statement not explicitly shown in part (iv) is that the proven statement shows that this function has no probability limit. However, it is trivial to see that for any candidate maximum $M$ and any $N>M$, we can draw some $X_i$ larger than $N$ with probability approaching 1 as we let $n\rightarrow \infty$, so it clearly is impossible for any such $M$ to be the probability limit of the maximum function.} The reason why we cannot apply LLN and CMT is because this function does not involve averages.%Take, for example, $X_i \sim N(0,1)$. For any candidate maximum, $x$, we can find an $\epsilon$ sufficiently small such that\begin{equation*} P\left(\left|\max_{1\leq i\leq n} \{X_i\} - x\right| \leq \epsilon\right) < 1-\epsilon.\end{equation*}

In most cases we can apply LLN and CMT to $1\{ \frac{1}{n}\sum_{i=1}^n X_i >0 \}$ so long as $E[X_i]\neq 0.$ If instead $E[X_i]= 0$, the function is not continuous at the relevant moment and therefore CMT does not apply.

\subsection{Part ii}
We can apply the central limit theorem and continuous mapping theorem to the first two as all of the transformations to the random variables are continuous. The first distribution is normal, and the second is a squared normal - that is, scaled chi-squared. However, the third expression is trivially 0. We can therefore not use the central limit theorem.
\subsection{Part iii}
Define $M_n:= \max_{1\leq i\leq n} X_i$ and let $X\sim uniform(0,1).$ Let $\epsilon>0$ be arbitrary. If $\epsilon\geq 1$ then $P(|M_n - 1| \leq \epsilon) = 1\geq 1-\epsilon$ so the definition of convergence in probability is trivially satisfied. Assume instead that $\epsilon \in (0,1)$.
\begin{align*}
P(|M_n - 1| \leq \epsilon) &= P(1-M_n \leq \epsilon)\\
&= 1- \prod_{i=1}^n P(X_i<1-\epsilon)\\
&= 1-(1-\epsilon)^n
\end{align*}

$1-(1-\epsilon)^n\rightarrow 1$ so $\exists N$ such that for all $n>N,1-(1-\epsilon)^n > 1-\epsilon .$ Therefore, for all $n>N$, $P(|M_n - 1| \leq \epsilon) = 1-(1-\epsilon)^n \geq 1-\epsilon$. Therefore, $M_n \rightarrow_p 1$. 
\subsection{Part iv}
As before, define $M_n:= \max_{1\leq i\leq n} X_i$  but now let $X\sim exponential(1).$ Let $M \geq 0$.
\begin{align*}
P(M_n>M) &= 1-P(M_n<M)\\
&= 1-\prod_{i=1}^n P(X_i<M)\\
&= 1- (1 - exp(-M))^n \\
&\rightarrow_{n\rightarrow \infty} 1.
\end{align*}
\section{Question 3}
\subsection{Part i}
By the central limit theorem,
\begin{align*}
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_i &= \frac{1}{\sqrt{n}}\sum_{i=1}^{n}(X_i - E[X_i])\\
&\rightarrow_d N(0,V)
\end{align*}
where $V = Var(X_i) = 1.$

\subsection{Part ii}
$E[Y_i] = E[X_i|W=1]P(W=1) +E[-X_i|W=-1]P(W=-1)  = 0(0.5) + 0(0.5) = 0.$\\
$E[Y_i^2] = E[X_i^2|W=1]P(W=1) + E[X_i^2|W=-1]P(W=-1) = 1(0.5) + 1(0.5) = 1.$
By the central limit theorem,
\begin{align*}
\frac{1}{\sqrt{n}}\sum_{i=1}^nY_i &= \frac{1}{\sqrt{n}}\sum_{i=1}^n(Y_i - E[Y_i])\\
\rightarrow_d N(0,W)
\end{align*}
where $W = Var(Y) = 1.$

\subsection{Part iii}
\begin{align*}
Cov(X_i,Y_i) &= E[X_iY_i] - E[X_i] E[Y_i]\\
&= E[X_i^2 W]\\
&= E[X_i^2]E[W]\\
&= 0.
\end{align*}
\subsection{Part iv}
\subsection{Part v}
% Leave optional questions for exam review.
\end{document}
